{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-3.0.3-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext(\"local\",\"Test App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-3ADCABBQ:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Test App>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=sc.parallelize(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkSession\n",
    "# spark=SparkSession.builder().master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To find out where the pyspark, there is no need to use the findspark module\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() #create spark session instance\n",
    "df = spark.sql(\"select 'spark' as hello\" )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create data in the form of list of tuples, each tuple is act as one row.\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "# create schema for this dataframe\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "#create DF from existing data & schema, we need to pass the schema here.\n",
    "df = spark.createDataFrame(data=data, schema = columns) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onesentense.txt MapPartitionsRDD[13] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# once we create a spark session internally it will create a spark context object but if eac seperate operation you can\n",
    "# create more instances of sc's\n",
    "# sc = SparkContext(\"local[1]\", \"second app\")\n",
    "text_file=sc.textFile(\"onesentense.txt\") # create RDD from text file.\n",
    "text_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:1.\tPrerequisite: & it appeared:1 times in file.\n",
      "word:You & it appeared:1 times in file.\n",
      "word:should & it appeared:1 times in file.\n",
      "word:have & it appeared:2 times in file.\n",
      "word:Java & it appeared:2 times in file.\n",
      "word:installed & it appeared:1 times in file.\n",
      "word:on & it appeared:2 times in file.\n",
      "word:your & it appeared:2 times in file.\n",
      "word:machine. & it appeared:1 times in file.\n",
      "word:If & it appeared:1 times in file.\n",
      "word:you & it appeared:1 times in file.\n",
      "word:don’t & it appeared:1 times in file.\n",
      "word:machine, & it appeared:1 times in file.\n",
      "word:please & it appeared:1 times in file.\n",
      "word:go & it appeared:1 times in file.\n",
      "word:to & it appeared:1 times in file.\n",
      "word:this & it appeared:1 times in file.\n",
      "word:site & it appeared:1 times in file.\n",
      "word:and & it appeared:1 times in file.\n",
      "word:download & it appeared:1 times in file.\n",
      "word:Java. & it appeared:1 times in file.\n"
     ]
    }
   ],
   "source": [
    "# we cant see the output of this transformation till we didnt perform any action over this counts.\n",
    "counts=text_file.flatMap(lambda line: line.split(\" \")).map(lambda word:(word,1)).reduceByKey(lambda a, b:a+b)\n",
    "# each line get splitted in words then map each word with value 1 to get aggregate value, then to get its count perform \n",
    "# reduceBYKey so same key count values get added\n",
    "\n",
    "# perform action to collect final result which has key & values pairs with key as respective word & value is its count in file.\n",
    "output=counts.collect()\n",
    "\n",
    "for (word,count) in output:\n",
    "    print(f\"word:{word} & it appeared:{count} times in file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tPrerequisite:: 1\n",
      "You: 1\n",
      "should: 1\n",
      "have: 2\n",
      "Java: 2\n",
      "installed: 1\n",
      "on: 2\n",
      "your: 2\n",
      "machine.: 1\n",
      "If: 1\n",
      "you: 1\n",
      "don’t: 1\n",
      "machine,: 1\n",
      "please: 1\n",
      "go: 1\n",
      "to: 1\n",
      "this: 1\n",
      "site: 1\n",
      "and: 1\n",
      "download: 1\n",
      "Java.: 1\n"
     ]
    }
   ],
   "source": [
    "# import findspark\n",
    "# findspark.init()# Creating Spark Context\n",
    "# from pyspark import SparkContext \n",
    "# for second operation we can create another instance\n",
    "# sc = SparkContext(\"local\", \"first app\")# Calculating words count\n",
    "text_file = sc.textFile(\"onesentense.txt\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "# Printing each word with its respective count\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))# Stopping Spark Context\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd1=sc.parallelize([1,2,3])\n",
    "res=rd1.reduce(lambda a,b:a*b) #here first a=1 & b=2, then a*b=2 now a=2,b=3 then a*b=6\n",
    "rd1.collect() #[1,2,3] this is RDD dataset\n",
    "res #6  this is aggregated result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 5, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd2=sc.parallelize([5,7,2,1,4])\n",
    "res1=rd2.takeOrdered(3,lambda s:-1*s) # 's' is each value from RDD\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "data=[1,2,3,4,5,6] #create RDD from python list\n",
    "rd2=sc.parallelize(data,4) # here we exteranally asking to create 4 partitions else it will create default no of partitions.\n",
    "type(rd2) #pyspark.rdd.RDD\n",
    "rd2.collect() #[1, 2, 3, 4, 5, 6]\n",
    "# we can convert RDD into Python list using collect(), as we seen in result indirectly its a list only.\n",
    "l1=rd2.collect()\n",
    "print(l1,type(l1))#[1, 2, 3, 4, 5, 6] <class 'list'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take unordered python list convert into RDD, again take 3 elements and then ordered in series.\n",
    "l2=[4,7,8,1,5,3,2,6,5]\n",
    "rd3=sc.parallelize(l2)\n",
    "rd33=rd3.takeOrdered(3,lambda v:v*1) #this function will return a list\n",
    "rd33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can store the RDD at \"/dbfs/FileStore/rdddata\" location in databricks\n",
    "# rd3.saveAsTextFile(\"/FileStore/rdddata\")\n",
    "\n",
    "# to see the stored file location we start shell and then get the list of files\n",
    "# %sh\n",
    "# ls -ltr /dbfs/FileStore/rdddata  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rd3.saveAsTextFile(\"D://Big Data//SavedRDDs\") \n",
    "# now this rdd is saved at \"D://Big Data//SavedRDDs\" location as \"part-00000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rd4.saveAsTextFile(\"D://Big Data//SavedRDDsInPartitions\") \n",
    "# this will save rdd at \"D://Big Data//SavedRDDsInPartitions\" location in 4 partitions as\n",
    "# part-00000, part-00001,part-00002,part-00003  ==> these file names are provided by default to control file names.\n",
    "# and also created 'crc' called as 'cyclic redundancy check'its basically a data verification method that computer uses to \n",
    "# check the accuracy of data on the disks(HDD, CD etc). \n",
    "# But when we create partitions in DBFS then it will create a partitions and also shows the access like read, write & all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As RDDs are immutable, so if you run a transformation then the new RDD is created. \n",
    "# And when you perform action then the RDD get converted into python sequences OR file OR print into user interface in output etc.\n",
    "# ** Lambda function take any no of elements from any no of collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length word is:\"is\" and max length word is: \"ever\"\n"
     ]
    }
   ],
   "source": [
    "# Reduce():=> To get one valued result over data. \n",
    "# EX: calculate the % of students. So whenever we perform reduce there is shuffle.\n",
    "\n",
    "#Pyspark code to find the least (min) length word from given list:\n",
    "words=['This','is','the','best','mac','ever']\n",
    "wordRdd=sc.parallelize(words)\n",
    "# here we can use the if-else in lambda function OR use the UDF within reduce()\n",
    "minword=wordRdd.reduce(lambda x,y:x if len(x)<len(y) else y) # 1st word saved in x & next in y and goes on.\n",
    "maxword=wordRdd.reduce(lambda x,y:x if len(x)>len(y) else y)\n",
    "print(f'Min length word is:\"{minword}\" and max length word is: \"{maxword}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Map():=> To perform one to one operation, one operation on one element at a time. One to one transformation.\n",
    "# No of inputs equal to no of outputs in map transformation.\n",
    "# Ex: increase the marks of English subject by 3 to each student then go for map. There is no shuffle.\n",
    "rd4=sc.parallelize(range(5,100,5))\n",
    "maprdd=rd4.map(lambda x:(x//5)*2) \n",
    "maprdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-940"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducerdd=rd4.reduce(lambda x,y:x-y)\n",
    "reducerdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 20,\n",
       " 30,\n",
       " 40,\n",
       " 50,\n",
       " 60,\n",
       " 70,\n",
       " 80,\n",
       " 90,\n",
       " 100,\n",
       " 110,\n",
       " 120,\n",
       " 130,\n",
       " 140,\n",
       " 150,\n",
       " 160,\n",
       " 170,\n",
       " 180,\n",
       " 190]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd4.map(lambda x:x*2).collect() # Transformation and action at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ever'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instead of lambda we can also use the UDF.\n",
    "def largerthan(x,y):\n",
    "    if len(x)>len(y): return x\n",
    "    elif len(x)<len(y): return y\n",
    "    else: # if both are equal, check lexicographically (alphabatical sequence)\n",
    "        if x>y:\n",
    "            return x\n",
    "        else:\n",
    "            return y\n",
    "largerthan('hello','words') #simple fun call\n",
    "wordRdd.reduce(largerthan) #this will call 'largerthan' UDF over wordRDD then from RDD this fun will takes 2 words at a time \n",
    "# as a input arguments, process it and return result and process goes on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Data is:[10, 20, 40, 50, 60] \n",
      "No of partitions are:4\n"
     ]
    }
   ],
   "source": [
    "rd4=sc.parallelize([10,20,40,50,60],4) #created 4 RDD\n",
    "data=rd4.collect()\n",
    "print(f\"RDD Data is:{data} \\nNo of partitions are:{rd4.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default no of partitions: 1\n",
      "After repartition the default becomes: 5\n",
      "After reducing partitions using coalesce the no of partitions are:3\n"
     ]
    }
   ],
   "source": [
    "# we can get the no of partitions using method\n",
    "rd5=sc.parallelize(range(100000))\n",
    "print(f\"Default no of partitions: {rd5.getNumPartitions()}\") # because of only one node it will create only 1 partition.\n",
    "new_rd5=rd5.repartition(5) # after repartition it will create new RDD\n",
    "print(f\"After repartition the default becomes: {new_rd5.getNumPartitions()}\")\n",
    "new_rd6=new_rd5.coalesce(3) #reduce the no of partitions\n",
    "print(f\"After reducing partitions using coalesce the no of partitions are:{new_rd6.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we doesn’t mentioned the no of partitions then by default it will create 8 partition for any kind of range value. \n",
    "# Because it has one property named as “sc.defaultparallelism” which has default value as 8 (In databricks its 8 because \n",
    "# databricks is spark on cluster and in jupyter notebook its 1 because jupyter notebook is on single node). \n",
    "# You may change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism #this will return default value of no of partitions it will create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of partitions: 12\n"
     ]
    }
   ],
   "source": [
    "rd7=sc.parallelize(range(12000000),numSlices=12)\n",
    "print(f\"No of partitions: {rd7.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Hive partitioning is at directory level where the partitions are created as folders & buckets are created as files, \n",
    "# hdfs level.\n",
    "# *** But in spark the partitions are at RDD level, RAM level, since there is no concept of bucketing here.\n",
    "# Here we can control the no of RDD for only this partition not for globally OR for whole cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: How do we decide that how many partition we needed?\n",
    "# ==>It’s a matter of subjective, so go with medium size of partitions.\n",
    "# ==>Ex: we have data 1GB data then go with 512MB OR 256MB partitions that is it will become 4 partitions. \n",
    "#     Its similar to block size.\n",
    "\n",
    "# ==>If we have only one partition then the linage and restart ability from the point of failure will not work because here \n",
    "#     all the partitions goes down, because only we have only partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: How to check how many nodes are working/involved for/in this processing?\n",
    "#==> It wont shows shuffle because of single node cluster. So these partitions are lying in the same node. \n",
    "# Because shuffle can happen within different nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MappedRdd: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      "ReducedRDD: 69 \n",
      "filteredRdd: [4, 8, 6, 8, 4, 2, 4]\n",
      "DistinctRdd: [4, 7, 5, 8, 6, 2, 3, 1]\n",
      "sorted 5 values are: [1, 2, 3, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "# map==> one to one transformation, No of inputs=No of outputs\n",
    "# reduce==> single valued result.\n",
    "# filter ==> Get only those who satisfy condition like even on from RDDs.\n",
    "# distinct ==> return only unique values from RDD.\n",
    "rd8=sc.parallelize([4,7,5,8,6,7,8,4,2,5,3,1,4,5])\n",
    "mappedRdd=rd8.map(lambda x:x//x).collect() # divide the value by its own\n",
    "reducedRdd=rd8.reduce(lambda x,y:x+y) # this will return singe value since its considered as int so collect() wont work \n",
    "# with reduce().\n",
    "filteredRdd=rd8.filter(lambda x:x%2==0).collect()\n",
    "distinctRdd=rd8.distinct().collect() # Used to remove the duplicate elements from RDD. \n",
    "sortedRdd=rd8.takeOrdered(5,lambda x:x*1) # This will return 5 elements in sorted order\n",
    "print(f\"MappedRdd: {mappedRdd} \\nReducedRDD: {reducedRdd} \\nfilteredRdd: {filteredRdd}\\nDistinctRdd: {distinctRdd}\\\n",
    "\\nsorted 5 values are: {sortedRdd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The no of elements in rd9 which are>3 are:[4, 5, 4, 5, 4, 5, 4, 5]\n",
      "The no of elements in rd9 which are>4 are:[5, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "n=10000\n",
    "# rd9=sc.parallelize([1,2,3,4,5]*(n//4)) #this will create same list (10000//4) times that is 2500 times in RDD.\n",
    "# rd9.collect()\n",
    "rd9=sc.parallelize([1,2,3,4,5]*4) # create same list 4 times in RDD\n",
    "def greaterthan(x):\n",
    "    return x>3\n",
    "print(f\"The no of elements in rd9 which are>3 are:{rd9.filter(greaterthan).collect()}\")\n",
    "print(f\"The no of elements in rd9 which are>4 are:{rd9.filter(lambda x:x>4).collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map: [['It', 'wont', 'shows', 'shuffle', 'because', 'of', 'single', 'node', 'cluster'], ['So', 'these', 'partitions', 'are', 'lying', 'in', 'the', 'same', 'node']]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "FlatMap: ['It', 'wont', 'shows', 'shuffle', 'because', 'of', 'single', 'node', 'cluster', 'So', 'these', 'partitions', 'are', 'lying', 'in', 'the', 'same', 'node']\n"
     ]
    }
   ],
   "source": [
    "# Difference between Map and FlatMap\n",
    "text=[\"It wont shows shuffle because of single node cluster\",\"So these partitions are lying in the same node\"]\n",
    "text_records_rdd=sc.parallelize(text)\n",
    "# map each line in text to a list of words\n",
    "print(f\"Map: {text_records_rdd.map(lambda line:line.split(' ')).collect()}\") # Map will create nested lists.\n",
    "print('--------------------------------------------------------------------------------------------------')\n",
    "\n",
    "# create a single list of words by combining all the words from above lines\n",
    "print(f\"FlatMap: {text_records_rdd.flatMap(lambda line:line.split(' ')).collect()}\")\n",
    "# FlatMap will create only one list of all words.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union as Bags (all values): [2, 4, 6, 8, 10, 1, 3, 2, 7, 9, 10]\n",
      "Union as sets (only unique values): [2, 4, 6, 8, 10, 1, 3, 7, 9]\n",
      "Union as sets in sorted order: [1, 2, 3, 4, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Example of union function in spark\n",
    "rd10=sc.parallelize([2,4,6,8,10])\n",
    "rd11=sc.parallelize([1,3,2,7,9,10])\n",
    "print(f\"Union as Bags (all values): {rd10.union(rd11).collect()}\")\n",
    "print(f\"Union as sets (only unique values): {rd10.union(rd11).distinct().collect()}\")\n",
    "print(f\"Union as sets in sorted order: {rd10.union(rd11).distinct().takeOrdered(10,lambda x:x*1)}\")\n",
    "# combine both RDD values, then collect unique records, then sort and take only 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can remove the duplicate elements from RDD\n",
    "DuplicateRdd=sc.parallelize([1,2,1,4,2,5,3,4,1,2,5])\n",
    "DuplicateRdd.collect()\n",
    "uniueRdd=DuplicateRdd.distinct().takeOrdered(5,lambda x:x*1)\n",
    "uniueRdd #this will return sorted unique Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101',\n",
       " 'amol',\n",
       " 'finance',\n",
       " '102',\n",
       " 'suraj',\n",
       " 'education',\n",
       " '103',\n",
       " 'mahesh',\n",
       " 'Agree']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interview Question \n",
    "rd11=sc.textFile(\"input.csv\")\n",
    "rd11.collect() \n",
    "#['101,amol,finance', '102,suraj,education', '103,mahesh,Agree'] here one row from file is act as one element in list.\n",
    "map_rdd=rd11.map(lambda line:line.split(',')) # now each element get splitted by ','\n",
    "map_rdd.collect() # This will create nested matrix (list of lists) of each line by split with ','.\n",
    "# [['101', 'amol', 'finance'],\n",
    "#  ['102', 'suraj', 'education'],\n",
    "#  ['103', 'mahesh', 'Agree']]\n",
    "\n",
    "flatmap_rdd=rd11.flatMap(lambda line:line.split(','))\n",
    "flatmap_rdd.collect() # this will return a single list of all the elements splitted with quoma.\n",
    "\n",
    "# ['101',\n",
    "#  'amol',\n",
    "#  'finance',\n",
    "#  '102',\n",
    "#  'suraj',\n",
    "#  'education',\n",
    "#  '103',\n",
    "#  'mahesh',\n",
    "#  'Agree']\n",
    "\n",
    "# Note: map() will create N-dimentional matrix but flatMap() will create single list of all the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coffee Panda', 'Happiest Panda Party', 'Happy Panda']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can create rdd form set also.\n",
    "rd12=sc.parallelize({\"Coffee Panda\",\"Happy Panda\",\"Happiest Panda Party\"})\n",
    "rd12.collect()\n",
    "# tokenize_rdd=rd12.map(tockenize)\n",
    "# tokenize_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "  ('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "rd13=sc.parallelize(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
